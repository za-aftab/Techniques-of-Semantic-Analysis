# Techniques-of-Semantic-Analysis
## Course Assignments Repository
This repository contains all the assignments completed for the Techniques of Semantic Analysis (TSA) course, completed in 2023. 

## Course objectives
This course was an introduction to vector representation of language for machine learning. It covered the following topics: distributional hypothesis, sparse vectors, dense vectors (word2vec), contextual vector representation (neural encoders) and more.

## Assignments Overview
Programming Assignment 1: Distributional vectors and similarity

This assignment focuses on understanding word vectors (embeddings) by implementing a Python script. The script takes raw text as input and generates word embeddings in a two-dimensional space, along with identifying the most similar words for each target word. Key steps include preprocessing text, calculating co-occurrence matrices, applying PCA for dimensionality reduction, and computing cosine similarity.

Programming Assignment 2: Classifying words with a perceptron

The objective of this exercise is to reinforce fundamental concepts of machine learning by implementing a perceptron classifier for a semantic task. The task involves classifying a set of words into two categories: those associated more with war and those associated more with peace. The provided training data, extracted from Leo Tolstoy's novel "War and Peace," consists of B-words (features) and T-words (objects of classification). The task is to develop a Python script that takes a co-occurrence matrix as input and determines the optimal weights for a sigmoid perceptron-like classifier. It's important to independently handle weight initialization, output calculation, error computation, weight updating, and define a stopping criterion.

Writing Assignment:

At the end of the course we had to submit short summaries on 6 topics discussed during the semester. The summaries should read as a coherent piece of text and it should demonstrate that you achieved the learning goals for that topic.

- Summary 1: Placing Words into Vector Spaces
- Summary 2: Calculating and Visualizing Word Similarity
- Summary 3: Prediction with a Single Neural Node (perceptron-like)
- Summary 4: A Plane Deep Network
- Summary 5: Language models as a representation of meaning
- Summary 6: Word2Vec

